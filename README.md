# ecommerce-dataops-pipeline

**ETL DataOps Pipeline for E-Commerce Sales** automated via GitHub Actions.

[![ETL: Extract & Transform](https://github.com/viniciosfernandess/ecommerce-dataops-pipeline/actions/workflows/extract_transform.yml/badge.svg)](https://github.com/viniciosfernandess/ecommerce-dataops-pipeline/actions/workflows/extract_transform.yml)
[![ETL: Load & Report](https://github.com/viniciosfernandess/ecommerce-dataops-pipeline/actions/workflows/load_report.yml/badge.svg)](https://github.com/viniciosfernandess/ecommerce-dataops-pipeline/actions/workflows/load_report.yml)
[![Python](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org/)

---

## ğŸš€ Quick Start (Local Execution)

1. **Clone the repo**:
   ```bash
   git clone https://github.com/viniciosfernandess/ecommerce-dataops-pipeline.git
   cd ecommerce-dataops-pipeline
   ```
2. **Create Python virtual environment**:
   ```bash
   python -m venv .venv
   # Windows (Git Bash)
   source .venv/Scripts/activate
   # macOS/Linux
   # source .venv/bin/activate
   pip install --upgrade pip
   pip install pandas sqlalchemy psycopg2-binary matplotlib requests python-dotenv
   ```
3. **Configure database connection**:
   - Create a `.env` file in project root:
     ```dotenv
     DATABASE_URL=postgresql://postgres:<YOUR_PASSWORD>@localhost:5432/ecommerce_db
     ```
   - Ensure PostgreSQL is running and schemas/tables are created (see **Setup** instructions).
4. **Run full pipeline**:
   ```bash
   python scripts/extract.py
   python scripts/transform.py
   python scripts/aggregate.py    # optional if aggregated step is separate
   python scripts/load.py
   python scripts/report.py
   ```

---

## âš™ï¸ GitHub Actions Automation

Two scheduled workflows orchestrate the ETL and reporting process:

- **ETL: Extract & Transform**
  - **Schedule**: daily at 00:00 BRT
  - **Steps**: extract CSV â†’ transform â†’ load into `staging.online_retail`

- **ETL: Load & Report**
  - **Schedule**: daily at 02:00 BRT
  - **Steps**: read staging â†’ compute metrics â†’ load into `analytics.sales_summary` â†’ export CSV â†’ generate PNG reports â†’ commit outputs back to repo

Workflows can also be manually triggered via **Actions** â†’ select workflow â†’ **Run workflow**.

---

## ğŸ“ Project Structure

```text
.
â”œâ”€â”€ .github/                # GitHub Actions workflows
â”‚   â””â”€â”€ workflows/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # Raw CSVs downloaded automatically
â”‚   â””â”€â”€ processed/         # Processed CSV metrics outputs
â”œâ”€â”€ scripts/               # Python ETL and reporting scripts
â”‚   â””â”€â”€ logs/              # Optional execution logs
â”œâ”€â”€ reports/               # Generated graph PNGs
â”œâ”€â”€ README.md              # Project documentation (this file)
â””â”€â”€ .gitignore             # Ignored files and folders
```

---

## ğŸ“Œ About This Project

This pipeline demonstrates skills in:

- Building a robust **ETL** process with Python and SQL
- Implementing **DataOps** automation with **GitHub Actions**
- Managing relational databases (**PostgreSQL**)
- Generating automated **analytics** and **visualizations**

